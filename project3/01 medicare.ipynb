{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helthcare provider fraud\n",
    "Data set from kaggle: https://www.kaggle.com/datasets/rohitrox/healthcare-provider-fraud-detection-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install graphdatascience pandas ipython numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "from graphdatascience import GraphDataScience\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j Sandbox Connection details\n",
    "DB_ULR = 'neo4j://localhost:7687'\n",
    "DB_USER = 'neo4j'\n",
    "DB_PASS = 'test1234'\n",
    "gds = GraphDataScience.from_neo4j_driver(DB_ULR, auth=(DB_USER, DB_PASS))\n",
    "gds.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provider\n",
    "train_provider_csv = pd.read_csv(\"./datasets/Train-1542865627584.csv\")\n",
    "train_provider_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Provider nodes\n",
    "gds.run_cypher('create constraint if not exists for (n:Provider) require (n.id) is node key')\n",
    "label_dist = gds.run_cypher('''\n",
    "    unwind $data as row\n",
    "    merge (n:Provider{id: row.Provider})\n",
    "        set n.fraud = case row.PotentialFraud when 'Yes' then true else false end\n",
    "    return n.fraud as is_fraud, count(*) as count\n",
    "''', params = {'data': train_provider_csv.to_dict('records')})\n",
    "label_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beneficiarydata\n",
    "pd.set_option('display.max_columns', None)\n",
    "train_beneficiary_csv = pd.read_csv(\"./datasets/Train_Beneficiarydata-1542865627584.csv\")\n",
    "train_beneficiary_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_beneficiary_csv.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.run_cypher('create constraint if not exists for (n:Condition) require (n.id) is node key')\n",
    "gds.run_cypher('''\n",
    "    unwind [\n",
    "        'RenalDiseaseIndicator',\n",
    "        'ChronicCond_Alzheimer', 'ChronicCond_Heartfailure',\n",
    "        'ChronicCond_KidneyDisease', 'ChronicCond_Cancer',\n",
    "        'ChronicCond_ObstrPulmonary', 'ChronicCond_Depression',\n",
    "        'ChronicCond_Diabetes', 'ChronicCond_IschemicHeart',\n",
    "        'ChronicCond_Osteoporasis', 'ChronicCond_rheumatoidarthritis',\n",
    "        'ChronicCond_stroke'\n",
    "    ] as conditionId\n",
    "    merge (n:Condition{id: conditionId})\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Beneficiary nodes and also has_condition relationships\n",
    "gds.run_cypher('create constraint if not exists for (n:Beneficiary) require (n.id) is node key')\n",
    "gds.run_cypher('''\n",
    "    match (c:Condition)\n",
    "    with collect(c) as conditions\n",
    "    unwind $data as row\n",
    "    merge (n:Beneficiary{id: row.BeneID})\n",
    "        set n.dob = date(row.DOB),\n",
    "            n.gender = row.Gender,\n",
    "            n.race = row.Race\n",
    "    with conditions, n, row\n",
    "    call {\n",
    "        with row, conditions, n\n",
    "        foreach(\n",
    "            c in [x in conditions where row[x.id] = 1 or row[x.id] = 'Y' | x] |\n",
    "            merge (n)-[:has_condition]->(c)\n",
    "        )\n",
    "    }\n",
    "''', params = {'data': train_beneficiary_csv.to_dict('records')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set date of death\n",
    "dead = train_beneficiary_csv[['BeneID','DOD']].dropna()\n",
    "gds.run_cypher(''' \n",
    "    unwind $data as row\n",
    "    match (n:Beneficiary{id: row.BeneID})\n",
    "        set n.dod = date(row.DOD)\n",
    "''', params = {'data': dead.to_dict('records')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute age of Beneficiaries\n",
    "agedist = gds.run_cypher(''' \n",
    "    with date() as today\n",
    "    match (n:Beneficiary)\n",
    "    set n.age = duration.between(n.dob, today).years\n",
    "    return n.age as age, count(*) as beneficiaries order by age\n",
    "''')\n",
    "agedist.hist(\"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpatientdata\n",
    "pd.set_option('display.max_columns', None)\n",
    "train_inpatient_csv = pd.read_csv(\"./datasets/Train_Inpatientdata-1542865627584.csv\")\n",
    "train_inpatient_csv.fillna( value=0, inplace=True)\n",
    "train_inpatient_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inpatient_csv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create claims data\n",
    "gds.run_cypher('create constraint if not exists for (n:Claim) require (n.id) is node key')\n",
    "gds.run_cypher('create constraint if not exists for (n:Provider) require (n.id) is node key')\n",
    "gds.run_cypher('create constraint if not exists for (n:Physician) require (n.id) is node key')\n",
    "gds.run_cypher('create constraint if not exists for (n:Diagnosis) require (n.id) is node key')\n",
    "gds.run_cypher('create constraint if not exists for (n:Procedure) require (n.id) is node key')\n",
    "\n",
    "claims_cypher = '''\n",
    "    unwind $data as row\n",
    "    merge (c:Claim{id: row.ClaimID})\n",
    "        set c.inpatient = $inpatient,\n",
    "            c.startDate = date(row.ClaimStartDt),\n",
    "            c.endDate = date(row.ClaimEndDt),\n",
    "            c.admissionDate = date(row.AdmissionDt),\n",
    "            c.dischargeDate = date(row.DischargeDt),\n",
    "            c.deductible_amt = row.DeductibleAmtPaid,\n",
    "            c.reimbursed_amt = row.InscClaimAmtReimbursed\n",
    "    merge (p:Provider{id: row.Provider})\n",
    "    merge (c)-[:provider]->(p)\n",
    "    merge (b:Beneficiary{id: row.BeneID})\n",
    "    merge (c)-[:beneficiary]->(b)\n",
    "    with c,p, row\n",
    "    call {\n",
    "        with c,p,row\n",
    "        with c,p,row where row.ClmAdmitDiagnosisCode <> 0\n",
    "        merge (d:Diagnosis{id: row.ClmAdmitDiagnosisCode})\n",
    "        merge (c)-[:admit_diagonisis]->(d)\n",
    "    }\n",
    "    call {\n",
    "        with c,p,row\n",
    "        with c,p,row where row.DiagnosisGroupCode <> 0\n",
    "        merge (d:Diagnosis{id: row.DiagnosisGroupCode})\n",
    "        merge (c)-[:diagonisis_group]->(d)\n",
    "    }\n",
    "    call {\n",
    "        with c,p,row\n",
    "        with c,p, [ x in  [ 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3',\n",
    "                            'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6',\n",
    "                            'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9',\n",
    "                            'ClmDiagnosisCode_10' ] where row[x] <> 0 |\n",
    "                   row[x]] as codes\n",
    "        foreach( code in codes | \n",
    "            merge (d:Diagnosis{id: code})\n",
    "            merge (c)-[:diagonisis_code]->(d)\n",
    "        )\n",
    "    }\n",
    "    call {\n",
    "        with c,p,row\n",
    "        with c,p, [ x in  [ 'ClmProcedureCode_1', 'ClmProcedureCode_2',\n",
    "                            'ClmProcedureCode_3', 'ClmProcedureCode_4', 'ClmProcedureCode_5',\n",
    "                            'ClmProcedureCode_6' ] where row[x] <> 0 |\n",
    "                   row[x]] as codes\n",
    "        foreach( code in codes | \n",
    "            merge (d:Procedure{id: code})\n",
    "            merge (c)-[:procedure_code]->(d)\n",
    "        )\n",
    "    }\n",
    "    call {\n",
    "        with c,p,row\n",
    "        with c,p,row where row.AttendingPhysician <> 0 \n",
    "        merge (ap:Physician{id: row.AttendingPhysician})\n",
    "        merge (c)-[:attending]->(ap)\n",
    "    }\n",
    "    call {\n",
    "        with c,p,row\n",
    "        with c,p,row where row.OperatingPhysician <> 0 \n",
    "        merge (op:Physician{id: row.OperatingPhysician})\n",
    "        merge (c)-[:operating]->(op)\n",
    "    }\n",
    "    call {\n",
    "        with c,p,row\n",
    "        with c,p,row where row.OtherPhysician <> 0\n",
    "        merge (ot:Physician{id: row.OtherPhysician})\n",
    "        merge (c)-[:other]->(ot)\n",
    "    }\n",
    "'''\n",
    "\n",
    "gds.run_cypher(claims_cypher, params = {'data': train_inpatient_csv.to_dict('records'), 'inpatient': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outpatientdata\n",
    "pd.set_option('display.max_columns', None)\n",
    "train_outpatient_csv = pd.read_csv(\"./datasets/Train_Outpatientdata-1542865627584.csv\")\n",
    "train_outpatient_csv.fillna( value=0, inplace=True)\n",
    "train_outpatient_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in np.array_split(train_outpatient_csv, 10):\n",
    "    gds.run_cypher(claims_cypher, params = {'data': chunk.to_dict('records'), 'inpatient': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph model so far\n",
    "![](./images/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some questions that we can investigate with cypher queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Billing for services that were not provided (Claim with no Procedure or Pysician)\n",
    "fake_invoice = gds.run_cypher(''' \n",
    "    match (c:Claim)\n",
    "    where not exists { (c)-->(:Procedure) }\n",
    "      and not exists { (c)-->(:Physician) }\n",
    "    return count(*) as numberOfClaims, sum(c.reimbursed_amt) as total_reimbursed_amt\n",
    "''')\n",
    "fake_invoice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Duplicate submission of a claim for the same service\n",
    "duplicate_submission =  gds.run_cypher(''' \n",
    "    match (c1:Claim)-->(:Procedure)<--(c2:Claim)\n",
    "    where id(c1)<id(c2)\n",
    "      and (c1)-[:beneficiary]->()<-[:beneficiary]-(c2)\n",
    "    return \n",
    "        c1.id as claim1, c2.id as claim2,\n",
    "        c1.reimbursed_amt as amt1, c2.reimbursed_amt as amt2,\n",
    "        [ (c1)-[:provider]->(p) | p.id][0] as provider1,\n",
    "        [ (c2)-[:provider]->(p) | p.id][0] as provider2,\n",
    "        [ (c1)-[:provider]->(p) | p.fraud][0] as provider1_fraud,\n",
    "        [ (c2)-[:provider]->(p) | p.fraud][0] as provider2_fraud\n",
    "\n",
    "    limit 10\n",
    "''')\n",
    "duplicate_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Misrepresenting the service provided (Beneficiares with similar diagonsis should have similar procedure cost)\n",
    "# Here, we assume that claims with similar diagonsis codes end up in the same community\n",
    "# So let's run louvain\n",
    "g_diagnosis, project_stats = gds.graph.project(\n",
    "    'g_diagnosis', \n",
    "    ['Claim', 'Diagnosis'], \n",
    "    ['diagonisis_code', 'diagonisis_group'])\n",
    "project_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.louvain.stats(g_diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gds.louvain.write(g_diagnosis, writeProperty='community_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_diagnosis.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our communities, what is the average claim amout, and what providers are above average\n",
    "gds.run_cypher('create range index if not exists for (n:Claim) on (n.community_id)')\n",
    "community_dist = gds.run_cypher(''' \n",
    "    match (n:Claim)\n",
    "    with n.community_id as community_id, \n",
    "            count(*) as number_of_claims,\n",
    "            avg(n.reimbursed_amt) as avg_community_amt\n",
    "        order by number_of_claims desc limit 50\n",
    "    match (c:Claim{community_id:community_id})-[:provider]->(p)\n",
    "    with p, community_id, avg_community_amt, \n",
    "            avg(c.reimbursed_amt) as avg_provider_amt\n",
    "        order by avg_provider_amt desc\n",
    "        where avg_provider_amt > avg_community_amt\n",
    "    with p, community_id, avg_community_amt, avg_provider_amt,\n",
    "        avg_provider_amt/avg_community_amt*100 as percent_over_average\n",
    "        order by  percent_over_average desc\n",
    "    return \n",
    "        community_id, \n",
    "        p.id as provider_id, \n",
    "        avg_community_amt, \n",
    "        avg_provider_amt,\n",
    "        percent_over_average,\n",
    "        p.fraud as is_fraud\n",
    "\n",
    "''')\n",
    "community_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Charging for a more complex or expensive service than was actually provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Billing for a covered service when the service actually provided was not covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep (need numeric label for our classes)\n",
    "gds.run_cypher(\"match (p:Provider) set p.fraud_label = case p.fraud when true then 1 else 0 end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we make a monopartite graph for embeddings?\n",
    "Excecise: Explore provider - provider relationships\n",
    "```cypher\n",
    "match p=(p1:Provider)<-[:provider]-()-->(py:Physician|Beneficiary)<--()-[:provider]->(p2:Provider)\n",
    "where id(p1)<id(p2)\n",
    "return p limit 50\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_train = gds.graph.get('g_train')\n",
    "g_train.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph projection\n",
    "g_train, project_stats = gds.graph.project.cypher(\n",
    "    'g_train',\n",
    "    'match (n:Provider) return id(n) as id, n.fraud_label as fraud_label, labels(n) AS labels',\n",
    "    ''' \n",
    "    match (p1:Provider)<-[:provider]-()-->(py:Physician|Beneficiary)<--()-[:provider]->(p2:Provider)\n",
    "    where p1<>p2\n",
    "    return id(p1) as source, id(p2) as target, count(*) as weight\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Graph projection\n",
    "# g_train, project_stats = gds.graph.project('g_train', \n",
    "# [\n",
    "#     { \n",
    "#         \"Provider\": {\"properties\": [\"fraud_label\"]},\n",
    "#         \"Condition\": {},\n",
    "#         \"Beneficiary\": {},\n",
    "#         \"Claim\": {},\n",
    "#         \"Physician\": {},\n",
    "#         \"Diagnosis\": {},\n",
    "#         \"Procedure\": {}\n",
    "#     } \n",
    "# ],\n",
    "# [\n",
    "#     {\n",
    "#         'has_condition': {'orientation': 'UNDIRECTED'},\n",
    "#         'provider': {'orientation': 'UNDIRECTED'},\n",
    "#         'attending': {'orientation': 'UNDIRECTED'},\n",
    "#         'beneficiary': {'orientation': 'UNDIRECTED'},\n",
    "#         'operating': {'orientation': 'UNDIRECTED'},\n",
    "#         'other': {'orientation': 'UNDIRECTED'},\n",
    "#         'admit_diagonisis': {'orientation': 'UNDIRECTED'},\n",
    "#         'diagonisis_group': {'orientation': 'UNDIRECTED'},\n",
    "#         'diagonisis_code': {'orientation': 'UNDIRECTED'},\n",
    "#         'procedure_code': {'orientation': 'UNDIRECTED'},\n",
    "#     }\n",
    "# ])\n",
    "# project_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gds.fastRP.write(g_train, embeddingDimension=2, writeProperty='embedding',relationshipWeightProperty='weight', iterationWeights=[0.0, 1.0, 1.0, 0.7, 0.7, 0.6, 0.6, 0.4, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline, _ = gds.beta.pipeline.nodeClassification.create(\"provider-fraud-pipe\")\n",
    "pipeline.addNodeProperty('fastRP', embeddingDimension=64, mutateProperty='embedding', relationshipWeightProperty='weight', iterationWeights=[0.0, 1.0, 1.0, 0.7, 0.7, 0.6, 0.6, 0.4, 0.4])\n",
    "pipeline.configureSplit(testFraction=0.3, validationFolds=5)\n",
    "pipeline.selectFeatures(['embedding'])\n",
    "pipeline.addLogisticRegression(tolerance=0.00001, maxEpochs=500, penalty=0.0, batchSize=32)\n",
    "pipeline.addMLP()\n",
    "pipeline.addRandomForest(maxDepth=20)\n",
    "pipeline.configureAutoTuning(maxTrials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipe_model, res = pipeline.train(g_train, modelName=\"fraud-model\", targetNodeLabels=['Provider'], targetProperty=\"fraud_label\", metrics=[\"ACCURACY\", \"F1_WEIGHTED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipe_model.metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipe_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = trained_pipe_model.predict_write(g_train, concurrency=8, writeProperty=\"predicted_label\", predictedProbabilityProperty=\"predicted_probablity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_pipe_model.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g_train.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "c_data = gds.run_cypher('''\n",
    "    match (n:Provider)\n",
    "    return n.fraud_label as actual, n.predicted_label as predicted\n",
    "''')\n",
    "c_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = pd.crosstab(c_data['actual'], c_data['predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
    "confusion_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('notebooks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n[Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68434ca16130d73e64183f54aeda16ecffce872c062f16976fd0d983519d848"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
